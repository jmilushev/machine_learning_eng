{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import ReplayBuffer, OUNoise\n",
    "\n",
    "buffer = ReplayBuffer(100, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Experience(state=[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], action=[0.0, 0.0, 0.0, 0.0], reward=0.8, next_state=[1.1, 2.1, 3.1, 4.1, 5.1, 6.1], done=1.0), Experience(state=[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], action=[0.0, 0.0, 0.0, 0.0], reward=0.8, next_state=[1.1, 2.1, 3.1, 4.1, 5.1, 6.1], done=1.0), Experience(state=[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], action=[0.0, 0.0, 0.0, 0.0], reward=0.8, next_state=[1.1, 2.1, 3.1, 4.1, 5.1, 6.1], done=1.0), Experience(state=[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], action=[0.0, 0.0, 0.0, 0.0], reward=0.8, next_state=[1.1, 2.1, 3.1, 4.1, 5.1, 6.1], done=1.0), Experience(state=[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], action=[0.0, 0.0, 0.0, 0.0], reward=0.8, next_state=[1.1, 2.1, 3.1, 4.1, 5.1, 6.1], done=1.0)]\n"
     ]
    }
   ],
   "source": [
    "buffer.add([1.,2.,3.,4.,5.,6.], [0.,0.,0.,0.], 0.8, [1.1,2.1,3.1,4.1,5.1,6.1], 1.)\n",
    "s = buffer.sample(1)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouNoise = OUNoise(10, 9, 0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.53792483 9.42714263 9.7254876  8.46322701 9.41099211 9.08371185\n",
      " 9.58096941 8.75160109 9.41718582 8.36783489]\n"
     ]
    }
   ],
   "source": [
    "s = ouNoise.sample()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8164965809277259\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v1 = np.array([2,3,4])\n",
    "v2 = np.array([3,4,6])\n",
    "\n",
    "\n",
    "e = np.sqrt(np.power(v1 - v2, 2).sum()) / len(v1)\n",
    "\n",
    "\n",
    "\n",
    "#e = np.sqrt(np.abs(np.power(v1,2) - np.power(v2, 2)).sum())\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import ReplayBuffer, OUNoise\n",
    "from models import Actor, Critic\n",
    "\n",
    "class DDPG():\n",
    "    \"\"\"Reinforcement Learning agent that learns using DDPG.\"\"\"\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "        self.state_size = task.state_size\n",
    "        self.action_size = task.action_size\n",
    "        self.action_low = task.action_low\n",
    "        self.action_high = task.action_high\n",
    "\n",
    "        # Actor (Policy) Model\n",
    "        self.actor_local = Actor(self.state_size, self.action_size, self.action_low, self.action_high)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, self.action_low, self.action_high)\n",
    "\n",
    "        # Critic (Value) Model\n",
    "        self.critic_local = Critic(self.state_size, self.action_size)\n",
    "        self.critic_target = Critic(self.state_size, self.action_size)\n",
    "\n",
    "        # Initialize target model parameters with local model parameters\n",
    "        self.critic_target.model.set_weights(self.critic_local.model.get_weights())\n",
    "        self.actor_target.model.set_weights(self.actor_local.model.get_weights())\n",
    "\n",
    "        # Noise process\n",
    "        self.exploration_mu = 0\n",
    "        self.exploration_theta = 0.15\n",
    "        self.exploration_sigma = 0.2\n",
    "        self.noise = OUNoise(self.action_size, self.exploration_mu, self.exploration_theta, self.exploration_sigma)\n",
    "\n",
    "        # Replay memory\n",
    "        self.buffer_size = 100000\n",
    "        self.batch_size = 64\n",
    "        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "\n",
    "        # Algorithm parameters\n",
    "        self.gamma = 0.99  # discount factor\n",
    "        self.tau = 0.01  # for soft update of target parameters\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.noise.reset()\n",
    "        state = self.task.reset()\n",
    "        self.last_state = state\n",
    "        return state\n",
    "\n",
    "    def step(self, action, reward, next_state, done):\n",
    "         # Save experience / reward\n",
    "        self.memory.add(self.last_state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "\n",
    "        # Roll over last state and action\n",
    "        self.last_state = next_state\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state(s) as per current policy.\"\"\"\n",
    "        state = np.reshape(state, [-1, self.state_size])\n",
    "        action = self.actor_local.model.predict(state)[0]\n",
    "        return list(action + self.noise.sample())  # add some noise for exploration\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\"\"\"\n",
    "        # Convert experience tuples to separate arrays for each element (states, actions, rewards, etc.)\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.array([e.action for e in experiences if e is not None]).astype(np.float32).reshape(-1, self.action_size)\n",
    "        rewards = np.array([e.reward for e in experiences if e is not None]).astype(np.float32).reshape(-1, 1)\n",
    "        dones = np.array([e.done for e in experiences if e is not None]).astype(np.uint8).reshape(-1, 1)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        #     Q_targets_next = critic_target(next_state, actor_target(next_state))\n",
    "        actions_next = self.actor_target.model.predict_on_batch(next_states)\n",
    "        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])\n",
    "\n",
    "        # Compute Q targets for current states and train critic model (local)\n",
    "        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)\n",
    "        self.critic_local.model.train_on_batch(x=[states, actions], y=Q_targets)\n",
    "\n",
    "        # Train actor model (local)\n",
    "        action_gradients = np.reshape(self.critic_local.get_action_gradients([states, actions, 0]), (-1, self.action_size))\n",
    "        self.actor_local.train_fn([states, action_gradients, 1])  # custom training function\n",
    "\n",
    "        # Soft-update target models\n",
    "        self.soft_update(self.critic_local.model, self.critic_target.model)\n",
    "        self.soft_update(self.actor_local.model, self.actor_target.model)   \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\"\"\"\n",
    "        local_weights = np.array(local_model.get_weights())\n",
    "        target_weights = np.array(target_model.get_weights())\n",
    "\n",
    "        assert len(local_weights) == len(target_weights), \"Local and target model parameters must have the same size\"\n",
    "\n",
    "        new_weights = self.tau * local_weights + (1 - self.tau) * target_weights\n",
    "        target_model.set_weights(new_weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quadcop",
   "language": "python",
   "name": "quadcop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
